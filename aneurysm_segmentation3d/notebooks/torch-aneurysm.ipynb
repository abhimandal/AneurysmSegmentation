{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "likely-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.base_dataset import BaseDataset\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "virgin-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from torch_geometric.io import read_txt_array\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "\n",
    "from plyfile import PlyData, PlyElement,PlyProperty, PlyListProperty\n",
    "\n",
    "import sys\n",
    "from omegaconf import OmegaConf\n",
    "import pyvista as pv\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "raising-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vanilla-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "pd.set_option(\"display.max_columns\", 2000)\n",
    "pd.set_option(\"display.max_colwidth\", 256)\n",
    "\n",
    "torch.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "compressed-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(8,6)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-finish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "popular-lunch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2_BC', '3_BC', '4_BC', '5_BM', '6_BM']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_id_list_new =  '2_BC\t3_BC\t4_BC\t5_BM\t6_BM\t7_BP\t8_BP\t9_KBW\t10_SUM\t11_DHM\t12_GAW\t13_PMM\t14_TR\t15_TR\t16_TR\t18_EM\t19_EM\t20_EM\t21_FA\t22_FA\t23_HJ\t24_HJ\t25_HM\t26_HM\t27_HM\t28_JM\t29_JM\t30_JM\t31_JM\t32_JM\t33_KBB\t34_KBB\t35_KBB\t36_KBB\t37_KBB\t38_KBB\t39_KBB\t40_KBB\t41_KBB\t42_KBB\t43_KBB\t44_AC\t45_AC\t46_LE\t47_LE\t48_LE\t50_LE\t51_LE\t52_LE\t54_MR\t55_MR\t56_WA\t57_SF\t58_SI\t59_SI'\n",
    "patient_id_list_new = patient_id_list_new.split()\n",
    "patient_id_list_new[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-valve",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "corporate-citation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Workspace\\\\Python\\\\torch-points3d\\\\notebooks'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "equal-tribune",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q44YjGjrvg2j"
   },
   "outputs": [],
   "source": [
    "DIR = os.getcwd() # Replace with your root directory, the data will go in DIR/data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "processed-richards",
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "JnLlPJ_dmzOV"
   },
   "source": [
    "#@title Configure the dataset {run: \"auto\"}\n",
    "CATEGORY = \"All\" #@param [\"Airplane\", \"Bag\", \"All\", \"Motorbike\"] {allow-input: true}\n",
    "USE_NORMALS = False #@param {type:\"boolean\"}|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "serial-mailing",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBRS6vxSvjRo"
   },
   "outputs": [],
   "source": [
    "shapenet_yaml = \"\"\"\n",
    "class: shapenet.ShapeNetDataset\n",
    "task: segmentation\n",
    "dataroot: %s\n",
    "first_subsampling: 0.02                       # Grid size of the input data\n",
    "pre_transforms:                               # Offline transforms, done only once\n",
    "    - transform: NormalizeScale           \n",
    "    - transform: GridSampling3D\n",
    "      params:\n",
    "        size: ${first_subsampling}\n",
    "train_transforms:                             # Data augmentation pipeline\n",
    "    - transform: RandomNoise\n",
    "      params:\n",
    "        sigma: 0.01\n",
    "        clip: 0.05\n",
    "    - transform: RandomScaleAnisotropic\n",
    "      params:\n",
    "        scales: [0.9,1.1]\n",
    "    - transform: AddOnes\n",
    "    - transform: AddFeatsByKeys\n",
    "      params:\n",
    "        list_add_to_x: [True]\n",
    "        feat_names: [\"ones\"]\n",
    "        delete_feats: [True]\n",
    "test_transforms:\n",
    "    - transform: AddOnes\n",
    "    - transform: AddFeatsByKeys\n",
    "      params:\n",
    "        list_add_to_x: [True]\n",
    "        feat_names: [\"ones\"]\n",
    "        delete_feats: [True]\n",
    "\"\"\" % (os.path.join(DIR,\"data\")) \n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "params = OmegaConf.create(shapenet_yaml)\n",
    "# if CATEGORY != \"All\":\n",
    "#     params.category = CATEGORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "liable-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params.raw_file_identifiers = patient_id_list_new\n",
    "# params.raw_file_identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-dividend",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "governmental-apollo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['2_BC', '3_BC', '4_BC', '5_BM', '6_BM', '7_BP', '9_KBW', '8_BP', '10_SUM', '11_DHM'], 'val': ['12_GAW', '13_PMM', '14_TR', '15_TR', '16_TR', '18_EM'], 'test': ['23_HJ', '24_HJ', '25_HM']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHUFFLED_SPLITS = {\n",
    "    \"train\": ['2_BC', '3_BC', '4_BC', '5_BM', '6_BM', '7_BP', '9_KBW', '8_BP', '10_SUM', '11_DHM'],\n",
    "    \"val\": ['12_GAW', '13_PMM', '14_TR', '15_TR', '16_TR', '18_EM'],\n",
    "    \"test\": ['23_HJ', '24_HJ', '25_HM'],\n",
    "}\n",
    "params.shuffled_splits = SHUFFLED_SPLITS\n",
    "params.shuffled_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "satisfactory-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_seg = {'aneur': [0,1,2,3,4]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "apart-toner",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e85366dd404110a3c67595028ab617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d33e4370b84905a8ff2c33e52e9494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2af67812d2c45d98075518d2b20906d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset: AneurysmDataset \n",
       "\u001b[0;95mtrain_pre_batch_collate_transform \u001b[0m= None\n",
       "\u001b[0;95mval_pre_batch_collate_transform \u001b[0m= None\n",
       "\u001b[0;95mtest_pre_batch_collate_transform \u001b[0m= None\n",
       "\u001b[0;95mpre_transform \u001b[0m= Compose([\n",
       "    NormalizeScale(),\n",
       "    GridSampling3D(grid_size=0.02, quantize_coords=False, mode=mean),\n",
       "])\n",
       "\u001b[0;95mtest_transform \u001b[0m= Compose([\n",
       "    AddOnes(),\n",
       "    AddFeatsByKeys(ones=True),\n",
       "])\n",
       "\u001b[0;95mtrain_transform \u001b[0m= Compose([\n",
       "    RandomNoise(sigma=0.01, clip=0.05),\n",
       "    RandomScaleAnisotropic([0.9, 1.1]),\n",
       "    AddOnes(),\n",
       "    AddFeatsByKeys(ones=True),\n",
       "])\n",
       "\u001b[0;95mval_transform \u001b[0m= None\n",
       "\u001b[0;95minference_transform \u001b[0m= Compose([\n",
       "    NormalizeScale(),\n",
       "    GridSampling3D(grid_size=0.02, quantize_coords=False, mode=mean),\n",
       "    AddOnes(),\n",
       "    AddFeatsByKeys(ones=True),\n",
       "])\n",
       "Size of \u001b[0;95mtrain_dataset \u001b[0m= 10\n",
       "Size of \u001b[0;95mtest_dataset \u001b[0m= 3\n",
       "Size of \u001b[0;95mval_dataset \u001b[0m= 6\n",
       "\u001b[0;95mBatch size =\u001b[0m None"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_points3d.datasets.segmentation import AneurysmDataset \n",
    "# dataset = AneurysmDataset.AneurysmDataset(params,category_to_seg )\n",
    "dataset = AneurysmDataset(params,category_to_seg )\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sustained-freeze",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Aneurysm(3)], Aneurysm(10))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.test_dataset, dataset.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "seven-blowing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(category=[7623], grid_size=[1], id_scan=[1], origin_id=[7623], pos=[7623, 3], x=[7623, 11], y=[7623])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "electronic-prime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(category=[10063], grid_size=[1], id_scan=[1], origin_id=[10063], pos=[10063, 3], x=[10063, 11], y=[10063])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.test_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-juvenile",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-ethics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "integrated-quick",
   "metadata": {
    "colab_type": "text",
    "id": "sLSGJVf30G1C"
   },
   "source": [
    "## Model for part segmentation\n",
    "Let's start by creating a multihead segmentation module with one segmentation head per category. We provide that as part of Torch Points3D but let's reproduce it here for sake of completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "civic-following",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4k79sQhzryp"
   },
   "outputs": [],
   "source": [
    "from torch_points3d.core.common_modules import MLP, UnaryConv\n",
    "\n",
    "class MultiHeadClassifier(torch.nn.Module):\n",
    "    \"\"\" Allows segregated segmentation in case the category of an object is known. \n",
    "    This is the case in ShapeNet for example.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features -\n",
    "        size of the input channel\n",
    "    cat_to_seg\n",
    "        category to segment maps for example:\n",
    "        {\n",
    "            'Airplane': [0,1,2],\n",
    "            'Table': [3,4]\n",
    "        }\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, cat_to_seg, dropout_proba=0.5, bn_momentum=0.1):\n",
    "        super().__init__()\n",
    "        self._cat_to_seg = {}\n",
    "        self._num_categories = len(cat_to_seg) #2\n",
    "        self._max_seg_count = 0\n",
    "        self._max_seg = 0\n",
    "        self._shifts = torch.zeros((self._num_categories,), dtype=torch.long)\n",
    "        for i, seg in enumerate(cat_to_seg.values()): #2 times for 2 keys\n",
    "            self._max_seg_count = max(self._max_seg_count, len(seg)) #overwriting every ith time\n",
    "            self._max_seg = max(self._max_seg, max(seg))\n",
    "            self._shifts[i] = min(seg)\n",
    "            self._cat_to_seg[i] = seg\n",
    "\n",
    "        self.channel_rasing = MLP(\n",
    "            [in_features, self._num_categories * in_features], bn_momentum=bn_momentum, bias=False\n",
    "        )\n",
    "        if dropout_proba:\n",
    "            self.channel_rasing.add_module(\"Dropout\", torch.nn.Dropout(p=dropout_proba))\n",
    "\n",
    "        self.classifier = UnaryConv((self._num_categories, in_features, self._max_seg_count))\n",
    "        self._bias = torch.nn.Parameter(torch.zeros(self._max_seg_count,))\n",
    "\n",
    "    def forward(self, features, category_labels, **kwargs):\n",
    "        assert features.dim() == 2\n",
    "        self._shifts = self._shifts.to(features.device)\n",
    "        in_dim = features.shape[-1]\n",
    "        features = self.channel_rasing(features)\n",
    "        features = features.reshape((-1, self._num_categories, in_dim))\n",
    "        features = features.transpose(0, 1)  # [num_categories, num_points, in_dim]\n",
    "        features = self.classifier(features) + self._bias  # [num_categories, num_points, max_seg]\n",
    "        ind = category_labels.unsqueeze(-1).repeat(1, 1, features.shape[-1]).long()\n",
    "\n",
    "        logits = features.gather(0, ind).squeeze(0)\n",
    "        softmax = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "        output = torch.zeros(logits.shape[0], self._max_seg + 1).to(features.device)\n",
    "        cats_in_batch = torch.unique(category_labels)\n",
    "        for cat in cats_in_batch:\n",
    "            cat_mask = category_labels == cat\n",
    "            seg_indices = self._cat_to_seg[cat.item()]\n",
    "            probs = softmax[cat_mask, : len(seg_indices)]\n",
    "            output[cat_mask, seg_indices[0] : seg_indices[-1] + 1] = probs\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-comfort",
   "metadata": {
    "colab_type": "text",
    "id": "vk2LyH-IT2KG"
   },
   "source": [
    "The model we implement here follows the main architecture proposed in the [original paper](https://arxiv.org/abs/1904.08889):\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"70%\" src=\"https://drive.google.com/uc?export=view&id=1CJppQ88T69whjYsJc016L3_E_rtcJ8n1\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "reserved-birmingham",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZnyHC6Gx1nIw"
   },
   "outputs": [],
   "source": [
    "from torch_points3d.applications.kpconv import KPConv\n",
    "\n",
    "\n",
    "class PartSegKPConv(torch.nn.Module):\n",
    "    def __init__(self, cat_to_seg):\n",
    "        super().__init__()\n",
    "        self.unet = KPConv(\n",
    "            architecture=\"unet\", \n",
    "            input_nc=10, \n",
    "            num_layers=4, \n",
    "            in_grid_size=0.0002\n",
    "            )\n",
    "        self.classifier = MultiHeadClassifier(self.unet.output_nc, cat_to_seg)\n",
    "    \n",
    "    @property\n",
    "    def conv_type(self):\n",
    "        \"\"\" This is needed by the dataset to infer which batch collate should be used\"\"\"\n",
    "        return self.unet.conv_type\n",
    "    \n",
    "    def get_batch(self):\n",
    "        return self.batch\n",
    "    \n",
    "    def get_output(self):\n",
    "        \"\"\" This is needed by the tracker to get access to the ouputs of the network\"\"\"\n",
    "        return self.output\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \"\"\" Needed by the tracker in order to access ground truth labels\"\"\"\n",
    "        return self.labels\n",
    "    \n",
    "    def get_current_losses(self):\n",
    "        \"\"\" Entry point for the tracker to grab the loss \"\"\"\n",
    "        return {\"loss_seg\": float(self.loss_seg)}\n",
    "\n",
    "    def forward(self, data):\n",
    "        self.labels = data.y\n",
    "        self.batch = data.batch\n",
    "        \n",
    "        # Forward through unet and classifier\n",
    "        data_features = self.unet(data)\n",
    "        self.output = self.classifier(data_features.x, data.category)\n",
    "\n",
    "         # Set loss for the backward pass\n",
    "        self.loss_seg = torch.nn.functional.nll_loss(self.output, self.labels)\n",
    "        return self.output\n",
    "\n",
    "    def get_spatial_ops(self):\n",
    "        return self.unet.get_spatial_ops()\n",
    "        \n",
    "    def backward(self):\n",
    "         self.loss_seg.backward() \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "classified-steel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = PartSegKPConv(dataset.class_to_segments)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "otherwise-updating",
   "metadata": {},
   "source": [
    "USE_NORMALS = True\n",
    "USE_NORMALS*3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-rehabilitation",
   "metadata": {
    "colab_type": "text",
    "id": "k9Bx8GjE3Kt7"
   },
   "source": [
    "## The data loaders and CPU pre computing features\n",
    "KPConv is quite demanding on spatial operations such as grid sampling and radius search. On the network loaded here we have 10 KPConv layers on the encoder which means 10 radius search operations with varying number of neighbours. We observed a significant performance gain by moving those operations to the CPU where they can easily be optimised with suitable data structures such as kd-tree. We use [nonaflann](https://github.com/jlblancoc/nanoflann) in the back-end, a 3D optimised kd-tree implementation. Note that this is beneficiary only if you have access to multiple CPU threads.\n",
    "\n",
    "You can decide to precompute those spatial operations by setting the `precompute_multi_scale` parameter to `True` when creating the data loaders. The dataset will mine the model to figure out which spatial operations are required and in which order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "different-discharge",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1k4CNG12RXN"
   },
   "outputs": [],
   "source": [
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE = 3\n",
    "dataset.create_dataloaders(\n",
    "    model,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=NUM_WORKERS, \n",
    "    shuffle=True, \n",
    "    precompute_multi_scale=True \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "artificial-fighter",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "id": "4wdS3y_aVIbm",
    "outputId": "5e564d1d-e966-41f3-a95f-ea830ee239f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x',\n",
       " 'y',\n",
       " 'pos',\n",
       " 'multiscale',\n",
       " 'upsample',\n",
       " 'batch',\n",
       " 'category',\n",
       " 'grid_size',\n",
       " 'id_scan',\n",
       " 'origin_id']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(dataset.train_dataloader))\n",
    "sample.keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-mediterranean",
   "metadata": {
    "colab_type": "text",
    "id": "A5pl2g9ap1Fq"
   },
   "source": [
    "Our `sample` contains the pre computed spatial information in the `multiscale` (encoder side) and `upsample` (decoder) attrivutes. The decoder pre computing is quite simple and just involves some basic caching for the nearest neighbour interpolation operation. Let's take a look at the encoder side of things first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "virtual-making",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 2, 2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "flexible-february",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "id": "z0o8yQF4FWvV",
    "outputId": "3a17a960-ea06-4037-e224-5e0853a48b9e",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Batch(batch=[27363], idx_neighboors=[27363, 25], pos=[27363, 3]),\n",
       " Batch(batch=[27363], idx_neighboors=[27363, 25], pos=[27363, 3]),\n",
       " Batch(batch=[27363], grid_size=[3], idx_neighboors=[27363, 25], pos=[27363, 3]),\n",
       " Batch(batch=[27363], idx_neighboors=[27363, 25], pos=[27363, 3]),\n",
       " Batch(batch=[27363], grid_size=[3], idx_neighboors=[27363, 25], pos=[27363, 3]),\n",
       " Batch(batch=[27363], idx_neighboors=[27363, 25], pos=[27363, 3]),\n",
       " Batch(batch=[27359], grid_size=[3], idx_neighboors=[27359, 25], pos=[27359, 3]),\n",
       " Batch(batch=[27359], idx_neighboors=[27359, 25], pos=[27359, 3]),\n",
       " Batch(batch=[27341], grid_size=[3], idx_neighboors=[27341, 25], pos=[27341, 3]),\n",
       " Batch(batch=[27341], idx_neighboors=[27341, 25], pos=[27341, 3])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.multiscale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-glenn",
   "metadata": {
    "colab_type": "text",
    "id": "B-2bjfQyqUyH"
   },
   "source": [
    "`sample.multiscale` contains 10 different versions of the input batch, each one of these versions contains the location of the points in `pos` as well as the neighbours of these points in the previous point cloud. We will first look at the points coming out of each downsampling layer (strided convolution), we have 5 of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-olive",
   "metadata": {
    "colab_type": "text",
    "id": "OsOeG_VXW5lC"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "mysterious-trash",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hg3O2swzW7if"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self,model, dataset, num_epoch = 3, device=torch.device('cpu')):\n",
    "        self.num_epoch = num_epoch\n",
    "        self._model = model\n",
    "        self._dataset=dataset\n",
    "        self.device = device\n",
    "\n",
    "    def fit(self):\n",
    "        self.optimizer = torch.optim.Adam(self._model.parameters(), lr=0.001)\n",
    "        self.tracker = self._dataset.get_tracker(False, True)\n",
    "\n",
    "        for i in range(self.num_epoch):\n",
    "            print(\"=========== EPOCH %i ===========\" % i)\n",
    "            time.sleep(0.5)\n",
    "            self.train_epoch()\n",
    "            self.tracker.publish(i)\n",
    "            self.test_epoch()\n",
    "            self.tracker.publish(i)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self._model.to(self.device)\n",
    "        self._model.train()\n",
    "        self.tracker.reset(\"train\")\n",
    "        train_loader = self._dataset.train_dataloader\n",
    "        iter_data_time = time.time()\n",
    "        with tqdm(train_loader) as tq_train_loader:\n",
    "            for i, data in enumerate(tq_train_loader):\n",
    "                t_data = time.time() - iter_data_time\n",
    "                iter_start_time = time.time()\n",
    "                self.optimizer.zero_grad()\n",
    "                data.to(self.device)\n",
    "                self._model.forward(data)\n",
    "                self._model.backward()\n",
    "                self.optimizer.step()\n",
    "                if i % 10 == 0:\n",
    "                    self.tracker.track(self._model)\n",
    "\n",
    "                tq_train_loader.set_postfix(\n",
    "                    **self.tracker.get_metrics(),\n",
    "                    data_loading=float(t_data),\n",
    "                    iteration=float(time.time() - iter_start_time),\n",
    "                )\n",
    "                iter_data_time = time.time()\n",
    "\n",
    "    def test_epoch(self):\n",
    "        self._model.to(self.device)\n",
    "        self._model.eval()\n",
    "        self.tracker.reset(\"test\")\n",
    "        test_loader = self._dataset.test_dataloaders[0]\n",
    "        iter_data_time = time.time()\n",
    "        with tqdm(test_loader) as tq_test_loader:\n",
    "            for i, data in enumerate(tq_test_loader):\n",
    "                t_data = time.time() - iter_data_time\n",
    "                iter_start_time = time.time()\n",
    "                data.to(self.device)\n",
    "                self._model.forward(data)           \n",
    "                self.tracker.track(self._model)\n",
    "\n",
    "                tq_test_loader.set_postfix(\n",
    "                    **self.tracker.get_metrics(),\n",
    "                    data_loading=float(t_data),\n",
    "                    iteration=float(time.time() - iter_start_time),\n",
    "                )\n",
    "                iter_data_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "american-hypothesis",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFaIG1SBchHg"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "worse-climate",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95,
     "referenced_widgets": [
      "8f58dfcaa0dd4dc0b4671c8350199388",
      "efaf0403696444919d2a454b1a77d28b",
      "d854f496052a4bf29a3459c487fe5304",
      "8d4e84720a394f44b0994e3788285878",
      "4504ad204d3241a6ba2f71cd4021a028",
      "e54acd47d4f8465da63ff2a5d425c42c",
      "3d23c649ccf7408c85905baa03a144aa",
      "93fbdcc2d2094c37a36c34b1d9c769df"
     ]
    },
    "colab_type": "code",
    "id": "ydhhP0CdcqO1",
    "outputId": "0888b8ae-e57d-4aac-e53b-e30067bc1dac",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== EPOCH 0 ===========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703ec05035b34f839cbdb08c92e133bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:18<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b53af344e404a45abb1b0e2084c7205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:18<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== EPOCH 1 ===========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffaa9e2deacc484db066597d7693da7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:22<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134017fbf5344ec58b2f307d70a23c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:22<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== EPOCH 2 ===========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458e941e4b8542e0966bfe983511b99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:21<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:75] data. DefaultCPUAllocator: not enough memory: you tried to allocate 854016000 bytes. Buy new RAM!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-b6cd7ba69c0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-a28c5cff357c>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"=========== EPOCH %i ===========\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtracker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpublish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-a28c5cff357c>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-43f74398153b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m# Forward through unet and classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mdata_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhil\\anaconda3\\envs\\kpconv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhil\\anaconda3\\envs\\kpconv\\lib\\site-packages\\torch_points3d\\applications\\kpconv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \"\"\"\n\u001b[0;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecomputed_down\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_computed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecomputed_up\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_mlp_head\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhil\\anaconda3\\envs\\kpconv\\lib\\site-packages\\torch_points3d\\models\\base_architectures\\unet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, data, precomputed_down, precomputed_up, **kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdown_modules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecomputed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecomputed_down\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[0mstack_down\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdown_modules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecomputed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecomputed_down\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner_modules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIdentity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhil\\anaconda3\\envs\\kpconv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhil\\anaconda3\\envs\\kpconv\\lib\\site-packages\\torch_points3d\\modules\\KPConv\\blocks.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, data, precomputed, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecomputed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecomputed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecomputed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhil\\anaconda3\\envs\\kpconv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhil\\anaconda3\\envs\\kpconv\\lib\\site-packages\\torch_points3d\\modules\\KPConv\\blocks.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, data, precomputed, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_bottleneck\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munary_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkp_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecomputed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecomputed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_bottleneck\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munary_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhil\\anaconda3\\envs\\kpconv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhil\\anaconda3\\envs\\kpconv\\lib\\site-packages\\torch_points3d\\modules\\KPConv\\blocks.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, data, precomputed, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mquery_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx_neighboors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midx_neighboors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkp_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_neighboors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhil\\anaconda3\\envs\\kpconv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhil\\anaconda3\\envs\\kpconv\\lib\\site-packages\\torch_points3d\\modules\\KPConv\\kernels.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, query_points, support_points, neighbors, x)\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoint_influence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKP_influence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregation_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         )\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_feat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhil\\anaconda3\\envs\\kpconv\\lib\\site-packages\\torch_points3d\\modules\\KPConv\\convolution_ops.py\u001b[0m in \u001b[0;36mKPConv_ops\u001b[1;34m(query_points, support_points, neighbors_indices, features, K_points, K_values, KP_extent, KP_influence, aggregation_mode)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;31m# Apply network weights [n_kpoints, n_points, out_fdim]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[0mweighted_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweighted_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mkernel_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweighted_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;31m# Convolution sum to get [n_points, out_fdim]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:75] data. DefaultCPUAllocator: not enough memory: you tried to allocate 854016000 bytes. Buy new RAM!"
     ]
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-forum",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
